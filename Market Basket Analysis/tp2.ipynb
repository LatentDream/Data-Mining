{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mh2MiuVMazk"
   },
   "source": [
    "# TP2 - Market Basket Analysis \n",
    "INF8111 - Fouille de données, Summer 2020\n",
    "### Membres de l'équipe\n",
    "    - Guillaume Thibault 1948612\n",
    "    - Marie-Eve Patron 1890562\n",
    "    - Julien Witty     1949837\n",
    "\n",
    "\n",
    "## Date et directives de remise\n",
    "Vous remettrez ce fichier nommé dans la boîte de remise sur moodle. \n",
    "\n",
    "\n",
    "## Market Basket Analysis\n",
    "\n",
    "Le *Market Basket Analysis* (MBA) est une technique d'analyse de la fouille de données qui permet de découvrir les associations entre les produits ou leur regroupement. En explorant des motifs intéressants à partir d'une vaste collection de données, le MBA vise à comprendre / révéler les comportements d'achat des clients en se basant sur la théorie selon laquelle si vous avez acheté un certain ensemble de produits, vous êtes plus (ou moins) susceptible d'acheter un autre groupe de produits. En d'autres termes, le MBA permet aux détaillants d'identifier la relation entre les articles que les clients achètent, révélant des tendances d'articles souvent achetés ensemble.\n",
    "\n",
    "Une approche largement utilisée pour explorer ces motifs consiste à construire *** des règles d'association *** telles que\n",
    "- **si** acheté *ITEM_1* **alors** achètera *ITEM_2* avec **confiance** *X*.\n",
    "\n",
    "Ces associations n'ont pas à être des règles individuelles. Ils peuvent impliquer de nombreux éléments. Par exemple, une personne dans un supermarché peut ajouter des œufs dans son panier, puis le MBA peut suggérer qu'elle achètera également du pain et/ou de la farine:\n",
    "\n",
    "+ **si**  acheté *OEUFS* **alors** achètera [*PAIN* avec confiance *0,2*; *FARINE* avec confiance 0,05].\n",
    "\n",
    "Cependant, si la personne décide maintenant d'ajouter de la farine à son panier, la nouvelle règle d'association pourrait être comme ci-dessous, suggérant des ingrédients pour faire un gâteau.\n",
    "\n",
    "+ **si** acheté [*OEUFS, FARINE*] **alors** achètera [*SUCRE* avec confiance 0,45; LEVURE avec confiance 0,12; *PAIN* avec confiance *0,03*].\n",
    "\n",
    "Il existe de nombreux scénarios réels où le MBA joue un rôle central dans l'analyse des données, comme les transactions de supermarché, les commandes en ligne ou l'historique des cartes de crédit. Les spécialistes du marketing peuvent utiliser ces règles d'association pour organiser les produits corrélés plus près les uns des autres sur les étagères des magasins ou faire des suggestions en ligne afin que les clients achètent plus d'articles. Un MBA peut généralement aider les détaillants à répondre aux questions les suivantes:\n",
    "\n",
    "- Quels articles sont souvent achetés ensemble ?\n",
    "- Étant donné un panier, quels articles suggérer ?\n",
    "- Comment placer les articles ensemble sur les étagères ?\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Votre objectif dans ce TP est de développer un algorithme MBA pour révéler les motifs en créant des règles d'association dans un ensemble de données volumineux avec plus de trois millions de transactions de supermarché. Cependant, la collecte de règles d'association dans les grands ensembles de données est un problème très intensif en calcul, ce qui rend presque impossible leur exécution sans système distribué. Par conséquent, pour exécuter votre algorithme, vous aurez accès à un cluster de *cloud computing* distribué avec des centaines de cœurs.\n",
    "\n",
    "À cette fin, un algorithme **MapReduce** sera implémenté avec le framework [Apache Spark](http://spark.apache.org), un système informatique distribué rapide. En résumé, Spark est un framework open source conçu avec une méthodologie *scale-out* qui en fait un outil très puissant pour les programmeurs ou les développeurs d'applications pour effectuer un volume massif de calculs et de traitement de données dans des environnements distribués. Spark fournit des API de haut niveau qui facilitent la création d'applications parallèles sans avoir à se soucier de la façon dont votre code et vos données sont parallélisés / distribués par le cluster informatique. Spark fait tout pour vous.\n",
    "\n",
    "La mise en œuvre suivra l'algorithme d'analyse du panier de marché présenté par Jongwook Woo et Yuhang Xu (2012). L'image **workflow.pdf** illustre le flux de travail de l'algorithme et doit être utilisée pour consultation tout au long de ce TP. Les cases bleues sont celles où vous devez implémenter une méthode pour effectuer une fonction de mappage ou de réduction, et les cases grises représentent leur sortie attendue. **Toutes ces opérations sont expliquées en détail dans les sections suivantes.**\n",
    "\n",
    "## 1. Configuration de Spark\n",
    "\n",
    "Spark fonctionne sur les systèmes Windows et UNIX (par exemple, Linux, Mac OS). Il est facile d'exécuter Spark localement sur une seule machine - tout ce dont vous avez besoin est d'avoir Java installé sur votre système PATH, ou la variable d'environnement JAVA_HOME pointant vers une installation Java. Il est obligatoire que le **JDK v8/11** soit installé sur votre système, car Spark ne prend actuellement en charge que cette version. Si ce n'est pas le cas, accédez à [la page Web de Java](https://www.oracle.com/java/technologies/javase-downloads.html) pour télécharger et installer une machine virtuelle Java. N'oubliez pas de définir la variable d'environnement JAVA_HOME pour utiliser JDK v8/11 si votre installation ne le fait pas automatiquement.\n",
    "\n",
    "L'interface entre Python et Spark se fait via **PySpark**, qui peut être installé en exécutant `pip install pyspark` ou configuré en suivant la séquence ci-dessous:\n",
    "\n",
    "1. D'abord, allez sur http://spark.apache.org/downloads\n",
    "2. Sélectionnez la dernière version de Spark et le package pré-construit pour Apache Hadoop 2.7\n",
    "3. Cliquez pour télécharger **spark-3.1.1-bin-hadoop2.7.tgz** et décompressez-le dans le dossier de votre choix.\n",
    "4. Ensuite, exportez les variables suivantes pour lier PYSPARK (l'interface python de Spark) à votre distribution python dans votre fichier `~/.bash_profile`.\n",
    "\n",
    "``\n",
    "export SPARK_HOME=/chemin/vers/spark-3.1.1-bin-hadoop2.7\n",
    "export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$PYTHONPATH\"\n",
    "export PYSPARK_PYTHON=/chemin/vers/votre/python3\n",
    "``\n",
    "\n",
    "5. Exécutez `source ~./bash_profile` pour effectuer les modifications et redémarrer cette session de notebook jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JmUMt4htMazm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Julie\\\\anaconda3\\\\python36.zip', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\lib', 'C:\\\\Users\\\\Julie\\\\anaconda3', '', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Julie\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Julie\\\\.ipython']\n",
      "C:\\Users\\Julie\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(sys.path)\n",
    "print(sys.executable)\n",
    "#os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# !a\"pt-get install openjdk-11-jdk-headless -qq > /dev/null\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rovSCW_vYs7m"
   },
   "source": [
    "#### Testez votre Spark\n",
    "À l'aide du code suivant, vous pouvez tester si Spark est installé correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UxgNiBFkYs7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaYpUpOFMazu"
   },
   "source": [
    "### 1.1 Exemple de comptage de produits \n",
    "\n",
    "Pour tester votre installation et commencer à vous familiariser avec Spark, nous suivrons un exemple qui compte combien de fois les produits d'un toy dataset ont été achetés.\n",
    "\n",
    "Le principal point d'entrée pour commencer la programmation avec Spark est [l'API RDD](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html#rdd-apis), une excellente abstraction Spark pour travailler avec MapReduce. RDD est une collection d'éléments partitionnés sur les nœuds du cluster qui peuvent fonctionner en parallèle. En d'autres termes, RDD est la façon dont Spark maintient vos données prêtes à exécuter une fonction (par exemple, une fonction Map ou une fonction reduce) en parallèle. **Ne vous inquiétez pas si cela semble toujours déroutant, il sera clair une fois que vous commencerez à l'implémenter**. Cependant, cela fait partie de ce TP d'étudier / consulter [Spark python API](https://spark.apache.org/docs/latest/api/python/) et d'apprendre à l'utiliser. Certaines fonctions utiles offertes par l'API RDD sont:\n",
    "\n",
    "1. **map**: return a new RDD by applying a function to each element of this RDD.\n",
    "2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n",
    "3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
    "4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n",
    "5. **groupByKey**: group the values for each key in the RDD into a single sequence\n",
    "6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n",
    "7. **sample**: return a sampled subset of this RDD\n",
    "8. **count**: return the number of elements in this RDD.\n",
    "9. **filter**: return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NZDz1nrBMazu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset\n",
      "+--------+-----------+\n",
      "|order_id|transaction|\n",
      "+--------+-----------+\n",
      "|       1|    a;b;c;f|\n",
      "|       2|    d;b;a;e|\n",
      "|       3|        c;b|\n",
      "|       4|        b;c|\n",
      "+--------+-----------+\n",
      "\n",
      "Toy dataframe as a RDD object (list of Row objects):\n",
      "\t [Row(order_id='1', transaction='a;b;c;f'), Row(order_id='2', transaction='d;b;a;e'), Row(order_id='3', transaction='c;b'), Row(order_id='4', transaction='b;c')]\n",
      "\n",
      "Mapped products:\n",
      "\t [('a', 1), ('b', 1), ('c', 1), ('f', 1), ('d', 1), ('b', 1), ('a', 1), ('e', 1), ('c', 1), ('b', 1), ('b', 1), ('c', 1)]\n",
      "\n",
      "Reduced (merged) products:\n",
      "\t [('a', 2), ('b', 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]\n",
      "\n",
      "Visualizing as a dataframe:\n",
      "+-------+-------------+\n",
      "|product|count_product|\n",
      "+-------+-------------+\n",
      "|      a|            2|\n",
      "|      b|            4|\n",
      "|      c|            3|\n",
      "|      f|            1|\n",
      "|      d|            1|\n",
      "|      e|            1|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def map_to_product(row):\n",
    "    \"\"\"\n",
    "    Map each transaction into a set of KEY-VALUE elements.\n",
    "    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n",
    "    \"\"\"\n",
    "    products = row.transaction.split(';') # split products from the column transaction\n",
    "    for p in products:\n",
    "        yield (p, 1)\n",
    "\n",
    "def reduce_product_by_key(value1, value2):\n",
    "    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n",
    "    return value1+value2\n",
    "\n",
    "# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "# Read a toy dataset\n",
    "toy = spark.read.csv('toy.csv', header=True)\n",
    "print(\"Toy dataset\")\n",
    "toy.show()\n",
    "\n",
    "# Obtain a RDD object to call a map function\n",
    "toy_rdd = toy.rdd\n",
    "print(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "# Map function to identify all products\n",
    "\n",
    "toy_rdd = toy_rdd.flatMap(map_to_product)\n",
    "list_toy = toy_rdd.collect()\n",
    "print(\"\\nMapped products:\\n\\t\", list_toy)\n",
    "\n",
    "# Reduce function to merge values of elements that share the same KEY\n",
    "toy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\n",
    "print(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n",
    "\n",
    "print(\"\\nVisualizing as a dataframe:\")\n",
    "toy_rdd.toDF([\"product\", \"count_product\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJGQmzXMazz"
   },
   "source": [
    "### 1.2 Travailler avec Spark Dataframe\n",
    "\n",
    "Dans l'exemple ci-dessus, nous avons brièvement utilisé une classe Dataframe de Spark, mais uniquement pour obtenir un objet RDD avec ``toy.rdd`` et pour aficher les données sous forme de tableau structuré avec le ``show ()`` une fonction. Cependant, [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#) est une partie cruciale de la version actuelle de Spark et est construit sur l'API RDD. Il s'agit d'une collection distribuée de lignes sous des colonnes nommées, identique à une table dans une base de données relationnelle. Le Dataframe de Spark fonctionne de la même manière que [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). En fait, nous pouvons exporter (obtenir) une Dataframe Spark vers (à partir de) ​​une Dataframe pandas avec la fonction ``toPandas()``  (``spark.createDataFrame``).\n",
    "\n",
    "Une fonctionnalité centrale du Dataframe est de bénéficier du [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), un module qui permet des requêtes SQL sur des données structurées. Par exemple, le même « exemple de comptage de produits » aurait pu être implémenté comme une séquence d'opérations SQL sur les données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oFL6BuIDMaz0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'products': exploding the transaction's products to a new row\n",
      "+--------+-----------+--------+\n",
      "|order_id|transaction|products|\n",
      "+--------+-----------+--------+\n",
      "|       1|    a;b;c;f|       a|\n",
      "|       1|    a;b;c;f|       b|\n",
      "|       1|    a;b;c;f|       c|\n",
      "|       1|    a;b;c;f|       f|\n",
      "|       2|    d;b;a;e|       d|\n",
      "|       2|    d;b;a;e|       b|\n",
      "|       2|    d;b;a;e|       a|\n",
      "|       2|    d;b;a;e|       e|\n",
      "|       3|        c;b|       c|\n",
      "|       3|        c;b|       b|\n",
      "|       4|        b;c|       b|\n",
      "|       4|        b;c|       c|\n",
      "+--------+-----------+--------+\n",
      "\n",
      "Couting unique products:\n",
      "+--------+-------------+\n",
      "|products|count_product|\n",
      "+--------+-------------+\n",
      "|       b|            4|\n",
      "|       c|            3|\n",
      "|       a|            2|\n",
      "|       d|            1|\n",
      "|       f|            1|\n",
      "|       e|            1|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Creates a new column, products, with all products appering in each transaction\n",
    "print('New column \\'products\\': exploding the transaction\\'s products to a new row')\n",
    "df_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\n",
    "df_toy.show()\n",
    "\n",
    "# Performs a select query and group rows by the product name, aggreagating by counting\n",
    "print('Couting unique products:')\n",
    "df_toy.select(df_toy.products)\\\n",
    "      .groupBy(df_toy.products)\\\n",
    "      .agg(f.count('products').alias('count_product'))\\\n",
    "      .sort('count_product', ascending=False)\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4HFs8CVMaz3"
   },
   "source": [
    "En outre, les mêmes opérations SQL effectuées ci-dessus auraient pu être effectuées avec une requête en langage SQL traditionnel comme indiqué ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O_eYl-7tMaz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|products|product_count|\n",
      "+--------+-------------+\n",
      "|       b|            4|\n",
      "|       c|            3|\n",
      "|       a|            2|\n",
      "|       f|            1|\n",
      "|       e|            1|\n",
      "|       d|            1|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a relational table TOY in the Spark session\n",
    "df_toy.createOrReplaceTempView(\"TOY\")\n",
    "\n",
    "spark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n",
    "          \" FROM TOY t\"\n",
    "          \" GROUP BY t.products\"\n",
    "          \" ORDER BY product_count DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0Onr2NxMaz8"
   },
   "source": [
    "Ces concepts SQL sont mentionnés ici car ils nous seront utiles lors du TP, principalement dans la section 3, pour manipuler les données du supermarché, qui sont structurées en tableaux. Ainsi, si vous n'êtes pas familier avec SQL, il est recommandé de suivre un [tutoriel](https://www.w3schools.com/sql/) pour comprendre les bases.\n",
    "\n",
    "## 2. Algorithme MBA\n",
    "Les sections suivantes expliquent comment développer chaque étape de l'algorithme MapReduce pour notre application de supermarché. La figure workflow.pdf illustre chaque étape de l'algorithme.\n",
    "\n",
    "### 2.1 Map to Patterns (10 points)\n",
    "Pour un sous-ensemble de transactions (c'est-à-dire les lignes de notre toy dataset), chaque transaction doit être **mappée** vers un ensemble de *motifs d'achat* trouvés dans la transaction. Formellement, ces motifs sont des sous-ensembles de produits qui représentent un groupe d'articles achetés ensemble. \n",
    "\n",
    "Pour le framework MapReduce, chaque motif doit être créé comme un élément *KEY-VALUE*, où la KEY peut prendre la forme d'un singleton, d'une paire ou d'un trio de produits présents dans la transaction. Plus précisément, pour chaque transaction, la fonction de mappage doit générer tous les sous-ensembles **UNIQUE** possibles de taille **UN, DEUX ou TROIS**. La VALEUR associée à chaque KEY est le nombre de fois que la KEY est apparue dans la transaction (si nous supposons qu'aucun produit n'apparaît plus d'une fois dans la transaction, cette valeur est toujours égale à un).\n",
    "\n",
    "Maintenant, implémentez la fonction **map_to_patterns** qui reçoit une transaction (une ligne du dataset) et retourne les motifs trouvés dans la transaction. Les éléments mappés sont un tuple (KEY, VALUE), où KEY est également un tuple de noms de produits. Il est crucial de noter que, puisque chaque entrée (transaction) de la fonction MAP produira **plus** un élément KEY-VALUE, un *flatMap* doit être invoqué pour cette étape.\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est similaire à:\n",
    "\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:1px\">\n",
    "<code>\n",
    "+---------------+-----------+\n",
    "|       patterns|occurrences|\n",
    "+---------------+-----------+\n",
    "|         ('a',)|          1|\n",
    "|     ('a', 'b')|          1|\n",
    "|('a', 'b', 'c')|          1|\n",
    "|('a', 'b', 'f')|          1|\n",
    "|     ('a', 'c')|          1|\n",
    "|('a', 'c', 'f')|          1|\n",
    "|     ('a', 'f')|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|('b', 'c', 'f')|          1|\n",
    "|     ('b', 'f')|          1|\n",
    "|         ('c',)|          1|\n",
    "|     ('c', 'f')|          1|\n",
    "|         ('f',)|          1|\n",
    "|         ('a',)|          1|\n",
    "|     ('a', 'b')|          1|\n",
    "|('a', 'b', 'd')|          1|\n",
    "|('a', 'b', 'e')|          1|\n",
    "|     ('a', 'd')|          1|\n",
    "|('a', 'd', 'e')|          1|\n",
    "|     ('a', 'e')|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'd')|          1|\n",
    "|('b', 'd', 'e')|          1|\n",
    "|     ('b', 'e')|          1|\n",
    "|         ('d',)|          1|\n",
    "|     ('d', 'e')|          1|\n",
    "|         ('e',)|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "|         ('b',)|          1|\n",
    "|     ('b', 'c')|          1|\n",
    "|         ('c',)|          1|\n",
    "+---------------+-----------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BscKKDAjMaz9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|       patterns|occurrences|\n",
      "+---------------+-----------+\n",
      "|         ('a',)|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('c',)|          1|\n",
      "|         ('f',)|          1|\n",
      "|     ('a', 'b')|          1|\n",
      "|     ('a', 'c')|          1|\n",
      "|     ('a', 'f')|          1|\n",
      "|     ('b', 'c')|          1|\n",
      "|     ('b', 'f')|          1|\n",
      "|     ('c', 'f')|          1|\n",
      "|('a', 'b', 'c')|          1|\n",
      "|('a', 'b', 'f')|          1|\n",
      "|('a', 'c', 'f')|          1|\n",
      "|('b', 'c', 'f')|          1|\n",
      "|         ('a',)|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('d',)|          1|\n",
      "|         ('e',)|          1|\n",
      "|     ('a', 'b')|          1|\n",
      "|     ('a', 'd')|          1|\n",
      "|     ('a', 'e')|          1|\n",
      "|     ('b', 'd')|          1|\n",
      "|     ('b', 'e')|          1|\n",
      "|     ('d', 'e')|          1|\n",
      "|('a', 'b', 'd')|          1|\n",
      "|('a', 'b', 'e')|          1|\n",
      "|('a', 'd', 'e')|          1|\n",
      "|('b', 'd', 'e')|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('c',)|          1|\n",
      "|     ('b', 'c')|          1|\n",
      "|         ('b',)|          1|\n",
      "|         ('c',)|          1|\n",
      "|     ('b', 'c')|          1|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def format_tuples(pattern):\n",
    "    \"\"\"\n",
    "    Used for visualizition.\n",
    "    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n",
    "    (a,b,c) -> '(a,b,c)'\n",
    "    \"\"\"\n",
    "    return (str(pattern[0]), str(pattern[1]))\n",
    "\n",
    "def map_to_patterns(row):\n",
    "    \"\"\"\n",
    "    Transforms row to combinaison of element\n",
    "        Yeild: Each combinaison\n",
    "    \"\"\"\n",
    "    element = row.transaction.split(';')\n",
    "    element.sort()\n",
    "    for e in itertools.combinations(element, 1):\n",
    "        yield Row(patterns=e, occurrences=1)\n",
    "    for e in itertools.combinations(element, 2):\n",
    "        yield Row(patterns=e, occurrences=1)\n",
    "    for e in itertools.combinations(element, 3):\n",
    "        yield Row(patterns=e, occurrences=1)\n",
    "        \n",
    "def map_to_patterns_from_list(row):\n",
    "    \"\"\"\n",
    "    Transforms row to combinaison of element\n",
    "        Yeild: Each combinaison\n",
    "    \"\"\"\n",
    "    element = row.transaction\n",
    "    element.sort()\n",
    "    for e in itertools.combinations(element, 1):\n",
    "        \n",
    "        yield Row(patterns=e, occurrences=1)\n",
    "    for e in itertools.combinations(element, 2):\n",
    "        yield Row(patterns=e, occurrences=1)\n",
    "    for e in itertools.combinations(element, 3):\n",
    "        yield Row(patterns=e, occurrences=1)\n",
    "\n",
    "toy_rdd = toy.rdd\n",
    "patterns_rdd = toy_rdd.flatMap(map_to_patterns)\n",
    "\n",
    "# Output as dataframe\n",
    "patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvvRw0plMa0B"
   },
   "source": [
    "### 2.2  Reduce patterns  (2,5 points)\n",
    "Une fois que différents processeurs ont traité les transactions, une fonction **reduce** doit être appelée pour combiner des KEYS identiques (le sous-ensemble de produits) et calculer le nombre total de ses occurrences dans le dataset. En d'autres termes, cette procédure de réduction doit additionner la *VALUE* de chaque KEY identique.\n",
    "\n",
    "Créez ci-dessous une fonction **reduce_patterns** qui doit additionner la VALUE de chaque motif.\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 28em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+--------------------+\n",
    "|       patterns|combined_occurrences|\n",
    "+---------------+--------------------+\n",
    "|         ('a',)|                   2|\n",
    "|     ('a', 'b')|                   2|\n",
    "|('a', 'b', 'c')|                   1|\n",
    "|('a', 'b', 'f')|                   1|\n",
    "|     ('a', 'c')|                   1|\n",
    "|('a', 'c', 'f')|                   1|\n",
    "|     ('a', 'f')|                   1|\n",
    "|         ('b',)|                   4|\n",
    "|     ('b', 'c')|                   3|\n",
    "|('b', 'c', 'f')|                   1|\n",
    "|     ('b', 'f')|                   1|\n",
    "|         ('c',)|                   3|\n",
    "|     ('c', 'f')|                   1|\n",
    "|         ('f',)|                   1|\n",
    "|('a', 'b', 'd')|                   1|\n",
    "|('a', 'b', 'e')|                   1|\n",
    "|     ('a', 'd')|                   1|\n",
    "|('a', 'd', 'e')|                   1|\n",
    "|     ('a', 'e')|                   1|\n",
    "|     ('b', 'd')|                   1|\n",
    "|('b', 'd', 'e')|                   1|\n",
    "|     ('b', 'e')|                   1|\n",
    "|         ('d',)|                   1|\n",
    "|     ('d', 'e')|                   1|\n",
    "|         ('e',)|                   1|\n",
    "+---------------+--------------------+\n",
    "</code>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "67IKY_4MMa0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|       patterns|combined_occurrences|\n",
      "+---------------+--------------------+\n",
      "|         ('a',)|                   2|\n",
      "|         ('b',)|                   4|\n",
      "|         ('c',)|                   3|\n",
      "|         ('f',)|                   1|\n",
      "|     ('a', 'b')|                   2|\n",
      "|     ('a', 'c')|                   1|\n",
      "|     ('a', 'f')|                   1|\n",
      "|     ('b', 'c')|                   3|\n",
      "|     ('b', 'f')|                   1|\n",
      "|     ('c', 'f')|                   1|\n",
      "|('a', 'b', 'c')|                   1|\n",
      "|('a', 'b', 'f')|                   1|\n",
      "|('a', 'c', 'f')|                   1|\n",
      "|('b', 'c', 'f')|                   1|\n",
      "|         ('d',)|                   1|\n",
      "|         ('e',)|                   1|\n",
      "|     ('a', 'd')|                   1|\n",
      "|     ('a', 'e')|                   1|\n",
      "|     ('b', 'd')|                   1|\n",
      "|     ('b', 'e')|                   1|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "combined_patterns_rdd = patterns_rdd.reduceByKey(add)\n",
    "\n",
    "# Output as dataframe\n",
    "combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BME1VugMa0F"
   },
   "source": [
    "### 2.3 Map to subpatterns (15 points)\n",
    "Ensuite, une autre fonction **map** doit être appliquée pour générer des sous-motifs. Encore une fois, les sous-motifs sont des éléments KEY-VALUE, où la KEY est également un sous-ensemble de produits. Cependant, la création de la KEY du sous-motif est une procédure différente. Cette fois, l'idée est de décomposer la liste des produits de chaque motif (KEY), de supprimer un produit à la fois et de produire la liste résultante en tant que nouvelle clé de sous-motif.\n",
    "\n",
    "Par exemple, pour un modèle donné $P$ avec trois produits, $p_1, p_2$ et $p_3$, trois nouvelles clés de sous-motifs vont être créées: (i) supprimer $p_1$ et retourner ($p_2, p_3$) ; (ii) supprimer $p_2$ et retourner ($p_1, p_3$); et (iii) supprimer $p_3$ et retourner ($p_1, p_2$).\n",
    "\n",
    "De plus, la structure VALUE du sous-motif sera également différente. Au lieu d'une seule valeur entière unique comme nous l'avons eu dans les motifs, cette fois un *tuple* devrait être créé pour le sous-motif VALUE. Ce tuple contient le produit qui a été retiré lors de la remise de la KEY et le nombre de fois que le motif est apparu. Par exemple ci-dessus, les valeurs doivent être ($p_1,v$), ($p_2,v$) et ($p_3,v $), respectivement, où $v$ est la VALEUR du motif.\n",
    "\n",
    "L'idée derrière les sous-motif est de créer **des règles** telles que : lorsque les produits de KEY ont été achetés, l'article présent dans la VALEUR a également été acheté *v* fois. En outre, chaque motif doit également produire un sous-motif dans lequel la clé est la même liste de produits du motif, mais la valeur est un tuple avec un produit nul (None) et le nombre de fois que le motif est apparu. Cet élément sera utile pour garder une trace du nombre de fois où un tel motif a été trouvé et sera utilisé ultérieurement pour calculer la valeur de confiance lors de la génération des règles d'association.\n",
    "\n",
    "**Implémentez la fonction map_to_subpatterns qui reçoit un motif et produit tous les sous-motif trouvés. Encore une fois, chaque entrée (motif) générera plus d'un élément KEY-VALUE, puis une fonction flatMap doit être appelée.**\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+---------+\n",
    "|    subpatterns|    rules|\n",
    "+---------------+---------+\n",
    "|         ('a',)|(None, 2)|\n",
    "|     ('a', 'b')|(None, 2)|\n",
    "|         ('b',)| ('a', 2)|\n",
    "|         ('a',)| ('b', 2)|\n",
    "|('a', 'b', 'c')|(None, 1)|\n",
    "|     ('b', 'c')| ('a', 1)|\n",
    "|     ('a', 'c')| ('b', 1)|\n",
    "|     ('a', 'b')| ('c', 1)|\n",
    "|('a', 'b', 'f')|(None, 1)|\n",
    "|     ('b', 'f')| ('a', 1)|\n",
    "|     ('a', 'f')| ('b', 1)|\n",
    "|     ('a', 'b')| ('f', 1)|\n",
    "|     ('a', 'c')|(None, 1)|\n",
    "|         ('c',)| ('a', 1)|\n",
    "|         ('a',)| ('c', 1)|\n",
    "|('a', 'c', 'f')|(None, 1)|\n",
    "|     ('c', 'f')| ('a', 1)|\n",
    "|     ('a', 'f')| ('c', 1)|\n",
    "|     ('a', 'c')| ('f', 1)|\n",
    "|     ('a', 'f')|(None, 1)|\n",
    "|         ('f',)| ('a', 1)|\n",
    "|         ('a',)| ('f', 1)|\n",
    "|         ('b',)|(None, 4)|\n",
    "|     ('b', 'c')|(None, 3)|\n",
    "|         ('c',)| ('b', 3)|\n",
    "|         ('b',)| ('c', 3)|\n",
    "|('b', 'c', 'f')|(None, 1)|\n",
    "|     ('c', 'f')| ('b', 1)|\n",
    "|     ('b', 'f')| ('c', 1)|\n",
    "|     ('b', 'c')| ('f', 1)|\n",
    "|     ('b', 'f')|(None, 1)|\n",
    "|         ('f',)| ('b', 1)|\n",
    "|         ('b',)| ('f', 1)|\n",
    "|         ('c',)|(None, 3)|\n",
    "|     ('c', 'f')|(None, 1)|\n",
    "|         ('f',)| ('c', 1)|\n",
    "|         ('c',)| ('f', 1)|\n",
    "|         ('f',)|(None, 1)|\n",
    "|('a', 'b', 'd')|(None, 1)|\n",
    "|     ('b', 'd')| ('a', 1)|\n",
    "|     ('a', 'd')| ('b', 1)|\n",
    "|     ('a', 'b')| ('d', 1)|\n",
    "|('a', 'b', 'e')|(None, 1)|\n",
    "|     ('b', 'e')| ('a', 1)|\n",
    "|     ('a', 'e')| ('b', 1)|\n",
    "|     ('a', 'b')| ('e', 1)|\n",
    "|     ('a', 'd')|(None, 1)|\n",
    "|         ('d',)| ('a', 1)|\n",
    "|         ('a',)| ('d', 1)|\n",
    "|('a', 'd', 'e')|(None, 1)|\n",
    "|     ('d', 'e')| ('a', 1)|\n",
    "|     ('a', 'e')| ('d', 1)|\n",
    "|     ('a', 'd')| ('e', 1)|\n",
    "|     ('a', 'e')|(None, 1)|\n",
    "|         ('e',)| ('a', 1)|\n",
    "|         ('a',)| ('e', 1)|\n",
    "|     ('b', 'd')|(None, 1)|\n",
    "|         ('d',)| ('b', 1)|\n",
    "|         ('b',)| ('d', 1)|\n",
    "|('b', 'd', 'e')|(None, 1)|\n",
    "|     ('d', 'e')| ('b', 1)|\n",
    "|     ('b', 'e')| ('d', 1)|\n",
    "|     ('b', 'd')| ('e', 1)|\n",
    "|     ('b', 'e')|(None, 1)|\n",
    "|         ('e',)| ('b', 1)|\n",
    "|         ('b',)| ('e', 1)|\n",
    "|         ('d',)|(None, 1)|\n",
    "|     ('d', 'e')|(None, 1)|\n",
    "|         ('e',)| ('d', 1)|\n",
    "|         ('d',)| ('e', 1)|\n",
    "|         ('e',)|(None, 1)|\n",
    "+---------------+---------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t8aLrdMuMa0G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|    subpatterns|    rules|\n",
      "+---------------+---------+\n",
      "|         ('a',)|(None, 2)|\n",
      "|         ('b',)|(None, 4)|\n",
      "|         ('c',)|(None, 3)|\n",
      "|         ('f',)|(None, 1)|\n",
      "|     ('a', 'b')|(None, 2)|\n",
      "|         ('b',)| ('a', 2)|\n",
      "|         ('a',)| ('b', 2)|\n",
      "|     ('a', 'c')|(None, 1)|\n",
      "|         ('c',)| ('a', 1)|\n",
      "|         ('a',)| ('c', 1)|\n",
      "|     ('a', 'f')|(None, 1)|\n",
      "|         ('f',)| ('a', 1)|\n",
      "|         ('a',)| ('f', 1)|\n",
      "|     ('b', 'c')|(None, 3)|\n",
      "|         ('c',)| ('b', 3)|\n",
      "|         ('b',)| ('c', 3)|\n",
      "|     ('b', 'f')|(None, 1)|\n",
      "|         ('f',)| ('b', 1)|\n",
      "|         ('b',)| ('f', 1)|\n",
      "|     ('c', 'f')|(None, 1)|\n",
      "|         ('f',)| ('c', 1)|\n",
      "|         ('c',)| ('f', 1)|\n",
      "|('a', 'b', 'c')|(None, 1)|\n",
      "|     ('b', 'c')| ('a', 1)|\n",
      "|     ('a', 'c')| ('b', 1)|\n",
      "|     ('a', 'b')| ('c', 1)|\n",
      "|('a', 'b', 'f')|(None, 1)|\n",
      "|     ('b', 'f')| ('a', 1)|\n",
      "|     ('a', 'f')| ('b', 1)|\n",
      "|     ('a', 'b')| ('f', 1)|\n",
      "|('a', 'c', 'f')|(None, 1)|\n",
      "|     ('c', 'f')| ('a', 1)|\n",
      "|     ('a', 'f')| ('c', 1)|\n",
      "|     ('a', 'c')| ('f', 1)|\n",
      "|('b', 'c', 'f')|(None, 1)|\n",
      "|     ('c', 'f')| ('b', 1)|\n",
      "|     ('b', 'f')| ('c', 1)|\n",
      "|     ('b', 'c')| ('f', 1)|\n",
      "|         ('d',)|(None, 1)|\n",
      "|         ('e',)|(None, 1)|\n",
      "|     ('a', 'd')|(None, 1)|\n",
      "|         ('d',)| ('a', 1)|\n",
      "|         ('a',)| ('d', 1)|\n",
      "|     ('a', 'e')|(None, 1)|\n",
      "|         ('e',)| ('a', 1)|\n",
      "|         ('a',)| ('e', 1)|\n",
      "|     ('b', 'd')|(None, 1)|\n",
      "|         ('d',)| ('b', 1)|\n",
      "|         ('b',)| ('d', 1)|\n",
      "|     ('b', 'e')|(None, 1)|\n",
      "|         ('e',)| ('b', 1)|\n",
      "|         ('b',)| ('e', 1)|\n",
      "|     ('d', 'e')|(None, 1)|\n",
      "|         ('e',)| ('d', 1)|\n",
      "|         ('d',)| ('e', 1)|\n",
      "|('a', 'b', 'd')|(None, 1)|\n",
      "|     ('b', 'd')| ('a', 1)|\n",
      "|     ('a', 'd')| ('b', 1)|\n",
      "|     ('a', 'b')| ('d', 1)|\n",
      "|('a', 'b', 'e')|(None, 1)|\n",
      "|     ('b', 'e')| ('a', 1)|\n",
      "|     ('a', 'e')| ('b', 1)|\n",
      "|     ('a', 'b')| ('e', 1)|\n",
      "|('a', 'd', 'e')|(None, 1)|\n",
      "|     ('d', 'e')| ('a', 1)|\n",
      "|     ('a', 'e')| ('d', 1)|\n",
      "|     ('a', 'd')| ('e', 1)|\n",
      "|('b', 'd', 'e')|(None, 1)|\n",
      "|     ('d', 'e')| ('b', 1)|\n",
      "|     ('b', 'e')| ('d', 1)|\n",
      "|     ('b', 'd')| ('e', 1)|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "def map_to_subpatterns(pattern):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Find all sub-pattern in the pattern\n",
    "    \"\"\"\n",
    "    print(f\"enter: {pattern}\")\n",
    "    products: tuple = pattern[0]\n",
    "    value: int = pattern[1]\n",
    "    print(f\"Yeild: {Row(subpatterns=deepcopy(products), combined_rules=(None, value))}\")\n",
    "    yield Row(subpatterns=deepcopy(products), combined_rules=(None, value))\n",
    "    if len(products) > 1:\n",
    "        for product in products:\n",
    "            i: int = products.index(product)\n",
    "            print(f\"Yeild: {Row(subpatterns=deepcopy(products[:i]) + deepcopy(products[i+1:]), combined_rules=(product, value))}\")\n",
    "            yield Row(subpatterns=deepcopy(products[:i]) + deepcopy(products[i+1:]), combined_rules=(product, value))\n",
    "\n",
    "\n",
    "\n",
    "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
    "\n",
    "# Output as dataframe\n",
    "subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl6TWh8rMa0J"
   },
   "source": [
    "### 2.4 Reduce Subpatterns (2.5 points)\n",
    "\n",
    "Encore une fois, une fonction **reduce** est nécessaire pour regrouper tous les sous-motif par leur KEY. L'objectif de cette procédure de réduction est de créer une liste de toutes les **règles** apparues dans KEY. Par conséquent, la sortie attendue résultant de cette fonction de réduction est également un élément KEY-VALUE, où la clé est la KEY du sous-motif et la valeur est un groupe contenant toutes les valeurs des sous-motif qui partagent la même clé.\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "\n",
    "\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 50em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+-------------------------------------------------------------+\n",
    "|subpatterns    |combined_rules                                               |\n",
    "+---------------+-------------------------------------------------------------+\n",
    "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n",
    "|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n",
    "|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n",
    "|('a', 'b', 'c')|[(None, 1)]                                                  |\n",
    "|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n",
    "|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n",
    "|('a', 'b', 'f')|[(None, 1)]                                                  |\n",
    "|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n",
    "|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n",
    "|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n",
    "|('a', 'c', 'f')|[(None, 1)]                                                  |\n",
    "|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n",
    "|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n",
    "|('b', 'c', 'f')|[(None, 1)]                                                  |\n",
    "|('a', 'b', 'd')|[(None, 1)]                                                  |\n",
    "|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n",
    "|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n",
    "|('a', 'b', 'e')|[(None, 1)]                                                  |\n",
    "|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n",
    "|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n",
    "+---------------+-------------------------------------------------------------+\n",
    "</code>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LOP-SVIhMa0J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------------------------------------------------+\n",
      "|subpatterns    |combined_rules                                               |\n",
      "+---------------+-------------------------------------------------------------+\n",
      "|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n",
      "|('b',)         |[(None, 4), ('a', 2), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n",
      "|('c',)         |[(None, 3), ('a', 1), ('b', 3), ('f', 1)]                    |\n",
      "|('f',)         |[(None, 1), ('a', 1), ('b', 1), ('c', 1)]                    |\n",
      "|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n",
      "|('a', 'c')     |[(None, 1), ('b', 1), ('f', 1)]                              |\n",
      "|('a', 'f')     |[(None, 1), ('b', 1), ('c', 1)]                              |\n",
      "|('b', 'c')     |[(None, 3), ('a', 1), ('f', 1)]                              |\n",
      "|('b', 'f')     |[(None, 1), ('a', 1), ('c', 1)]                              |\n",
      "|('c', 'f')     |[(None, 1), ('a', 1), ('b', 1)]                              |\n",
      "|('a', 'b', 'c')|[(None, 1)]                                                  |\n",
      "|('a', 'b', 'f')|[(None, 1)]                                                  |\n",
      "|('a', 'c', 'f')|[(None, 1)]                                                  |\n",
      "|('b', 'c', 'f')|[(None, 1)]                                                  |\n",
      "|('d',)         |[(None, 1), ('a', 1), ('b', 1), ('e', 1)]                    |\n",
      "|('e',)         |[(None, 1), ('a', 1), ('b', 1), ('d', 1)]                    |\n",
      "|('a', 'd')     |[(None, 1), ('b', 1), ('e', 1)]                              |\n",
      "|('a', 'e')     |[(None, 1), ('b', 1), ('d', 1)]                              |\n",
      "|('b', 'd')     |[(None, 1), ('a', 1), ('e', 1)]                              |\n",
      "|('b', 'e')     |[(None, 1), ('a', 1), ('d', 1)]                              |\n",
      "+---------------+-------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_rules = subpatterns_rdd.map(lambda row: (row.subpatterns, [ row.combined_rules ])).reduceByKey(add)\n",
    "\n",
    "\n",
    "# Output as dataframe\n",
    "combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh69x3a8Ma0P"
   },
   "source": [
    "### 2.5. Map to Association Rules (15 points)\n",
    "\n",
    "Enfin, la dernière étape de l'algorithme consiste à créer les règles d'association pour effectuer la MBA. Le but de cette fonction Map est de calculer le niveau **de confiance** de l'achat d'un produit, sachant qu'il y a déjà un ensemble de produits dans le panier. Ainsi, la KEY du sous-motif est l'ensemble des produits placés dans le panier et, pour chaque produit présent dans la liste des règles, c'est-à-dire dans la VALEUR, la confiance peut être calculée comme :\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\text{nombre de fois où le produit a été acheté avec KEY}}{\\text{nombre de fois où la KEY est apparue}}\n",
    "\\end{align*}\n",
    "\n",
    "Pour l'exemple donné dans la figure \"workflow\", *le café* a été acheté 20 fois et, dans 17 d'entre eux, le *lait* a été acheté ensemble. Ensuite, le niveau de confiance pour acheter du *lait* sachant que *le café* est dans le panier est $\\frac{17}{20}=0,85$, ce qui signifie que dans 85% des cas où le café a été acheté, le lait a aussi été acheté.\n",
    "\n",
    "Implémentez la fonction **map_to_assoc_rules** qui calcule le niveau de confiance pour chaque sous-motif.\n",
    "\n",
    "Pour le toy dataset, la sortie attendue est:\n",
    "<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 57em; padding-left:5px\">\n",
    "<code>\n",
    "+---------------+------------------------------------------------------------------+\n",
    "|patterns       |association_rules                                                 |\n",
    "+---------------+------------------------------------------------------------------+\n",
    "|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n",
    "|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n",
    "|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n",
    "|('a', 'b', 'c')|[]                                                                |\n",
    "|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n",
    "|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n",
    "|('a', 'b', 'f')|[]                                                                |\n",
    "|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n",
    "|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n",
    "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n",
    "|('a', 'c', 'f')|[]                                                                |\n",
    "|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
    "|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n",
    "|('b', 'c', 'f')|[]                                                                |\n",
    "|('a', 'b', 'd')|[]                                                                |\n",
    "|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n",
    "|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n",
    "|('a', 'b', 'e')|[]                                                                |\n",
    "|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n",
    "|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n",
    "+---------------+------------------------------------------------------------------+\n",
    "</code>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DPrbn5CfMa0P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------------------------------------------------------+\n",
      "|patterns       |association_rules                                                 |\n",
      "+---------------+------------------------------------------------------------------+\n",
      "|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n",
      "|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n",
      "|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n",
      "|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n",
      "|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n",
      "|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n",
      "|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n",
      "|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n",
      "|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n",
      "|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n",
      "|('a', 'b', 'c')|[]                                                                |\n",
      "|('a', 'b', 'f')|[]                                                                |\n",
      "|('a', 'c', 'f')|[]                                                                |\n",
      "|('b', 'c', 'f')|[]                                                                |\n",
      "|('d',)         |[('a', 1.0), ('b', 1.0), ('e', 1.0)]                              |\n",
      "|('e',)         |[('a', 1.0), ('b', 1.0), ('d', 1.0)]                              |\n",
      "|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n",
      "|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n",
      "|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n",
      "|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n",
      "+---------------+------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_to_assoc_rules(rule):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    pattern = rule[0]\n",
    "    rules = rule[1]\n",
    "    index = rules.index([item for item in rules if item[0] == None][0])\n",
    "    numTimeBought = rules.pop(index)[1]\n",
    "    newListOfRules = []\n",
    "    for element in rules:\n",
    "        newListOfRules.append((element[0], element[1] / numTimeBought))\n",
    "    yield Row(patterns=pattern, association_rules=newListOfRules)\n",
    "    \n",
    "    \n",
    "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
    "\n",
    "# Output as dataframe\n",
    "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPV5g2hwMa0U"
   },
   "source": [
    "## 3. Instacart dataset\n",
    "\n",
    "Avec votre algorithme MBA prêt à être utilisé, il est maintenant temps de travailler sur l'ensemble de données réel. Pour cette partie du TP, téléchargez le dataset [instacart](https://www.dropbox.com/s/qa7nsw2at3hsbmp/instacart.zip?dl=0) et lisez sa [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) pour comprendre la structure de l'ensemble de données.\n",
    "\n",
    "Avant d'appliquer l'algorithme développé sur l'ensemble de données instacart, vous devez d'abord filtrer les transactions pour qu'elles soient au même format défini par votre algorithme (une transaction par ligne). Pour manipuler les données, nous pouvons utiliser le bloc de données de Spark et le module SQL présenté dans la section 1.\n",
    "\n",
    "La cellule de code suivante utilise le module Spark SQL pour lire les commandes de ``order_products__train.csv`` et les informations détaillées de ``orders.csv`` et ``products.csv`` pour construire une dataframe qui contient un liste de tous les produits jamais achetés par chaque utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6oB1eTkeMa0W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_products__train.csv\n",
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|       1|     49302|                1|        1|\n",
      "|       1|     11109|                2|        1|\n",
      "|       1|     10246|                3|        0|\n",
      "|       1|     49683|                4|        0|\n",
      "|       1|     43633|                5|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orders.csv\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 2539329|      1|   prior|           1|        2|                8|                  null|\n",
      "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "products.csv\n",
      "+----------+--------------------+--------+-------------+\n",
      "|product_id|        product_name|aisle_id|department_id|\n",
      "+----------+--------------------+--------+-------------+\n",
      "|         1|Chocolate Sandwic...|      61|           19|\n",
      "|         2|    All-Seasons Salt|     104|           13|\n",
      "|         3|Robust Golden Uns...|      94|            7|\n",
      "|         4|Smart Ones Classi...|      38|            1|\n",
      "|         5|Green Chile Anyti...|       5|           13|\n",
      "+----------+--------------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------------------------------------------------------------------+\n",
      "|user_id|                                                                        products|\n",
      "+-------+--------------------------------------------------------------------------------+\n",
      "|      1|[Soda, Organic String Cheese, 0% Greek Strained Yogurt, XL Pick-A-Size Paper ...|\n",
      "|      2|[Organic Roasted Turkey Breast, Gluten Free Whole Grain Bread, Plantain Chips...|\n",
      "|      5|[Organic Raw Agave Nectar, Organic Large Extra Fancy Fuji Apple, Sharp Chedda...|\n",
      "|      7|[Panama Peach Antioxidant Infusion, Antioxidant Infusions Beverage Malawi Man...|\n",
      "|      8|[Shallot, Organic SprouTofu Silken Tofu, Nutritional Yeast Seasoning, Organic...|\n",
      "+-------+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_order_prod = spark.read.csv('instacart/order_products__train.csv', header=True, sep=',', inferSchema=True)\n",
    "print('order_products__train.csv')\n",
    "df_order_prod.show(5)\n",
    "\n",
    "df_orders = spark.read.csv('instacart/orders.csv', header=True, sep=',', inferSchema=True)\n",
    "print('orders.csv')\n",
    "df_orders.show(5)\n",
    "\n",
    "df_products = spark.read.csv('instacart/products.csv', header=True, sep=',', inferSchema=True)\n",
    "print('products.csv')\n",
    "df_products.show(5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "List of products ever purchased by each user\n",
    "\"\"\"\n",
    "# USING SQL\n",
    "df_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\n",
    "df_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\n",
    "df_products.createOrReplaceTempView(\"products\") # creates table 'products'\n",
    "spark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n",
    "               ' FROM orders o '\n",
    "               ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n",
    "               ' INNER JOIN products p    ON op.product_id = p.product_id'\n",
    "               ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n",
    "\n",
    "\n",
    "# USING DATAFRAME OPERATIONS\n",
    "# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n",
    "# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n",
    "# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n",
    "# .orderBy(df_orders.user_id).show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEqVeqhkMa0a"
   },
   "source": [
    "### 3.1 Perspectives commerciales (25 points) \n",
    "\n",
    "Maintenant, vous êtes le *data scientist*. En ne considérant que les commandes de ``order_products__train.csv``, l'utilisation du module Spark SQL, performant avec SQL ou dataframe, pour répondre aux questions suivantes:\n",
    "\n",
    "1. Quels sont les 10 produits les plus susceptibles d'être commandé de nouveau? Ne considérez que les produits achetés au moins 40 fois pour cette tâche.\n",
    "2. Quels sont les 3 produits les plus achetés dans chaque département?\n",
    "4. Quelle est la taille moyenne du panier pour chaque jour de la semaine?\n",
    "    - utilisez un barplot pour visualiser vos résultats\n",
    "\n",
    "**La sortie de ces questions doit contenir le NOM des produits, pas leur ID.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+----------------+\n",
      "|          product_name|numTimeBought|numTimeReordered|\n",
      "+----------------------+-------------+----------------+\n",
      "|                Banana|        92870|           16557|\n",
      "|Bag of Organic Bananas|        73739|           13362|\n",
      "|  Organic Strawberries|        87338|            8603|\n",
      "|  Organic Baby Spinach|        73237|            8055|\n",
      "|       Organic Avocado|        47549|            6226|\n",
      "|  Organic Hass Avocado|        50853|            6042|\n",
      "|           Large Lemon|        65595|            5923|\n",
      "|          Strawberries|        48324|            4786|\n",
      "|   Organic Raspberries|        42310|            4279|\n",
      "|                 Limes|        54180|            4234|\n",
      "+----------------------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Quels sont les 10 produits les plus susceptibles d'être commandé de nouveau? Ne considérez que les produits achetés au moins 40 fois pour cette tâche.\n",
    "\n",
    "spark.sql(\n",
    "    'SELECT p.product_name, SUM(op.add_to_cart_order) AS numTimeBought , SUM(op.reordered) / SUM(op.add_to_cart_order) AS numTimeReordered'\n",
    "                ' FROM products p'\n",
    "                ' INNER JOIN order_prod op ON p.product_id = op.product_id'\n",
    "                ' GROUP BY p.product_name HAVING numTimeBought >= 40 ORDER BY numTimeReordered DESC'\n",
    ").show(10, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|        product_name|department_id|\n",
      "+--------------------+-------------+\n",
      "|Sparkling Water G...|            7|\n",
      "|Lime Sparkling Water|            7|\n",
      "|Sparkling Natural...|            7|\n",
      "| Organic Black Beans|           15|\n",
      "|No Salt Added Bla...|           15|\n",
      "|Organic Garbanzo ...|           15|\n",
      "|  Lavender Hand Soap|           11|\n",
      "|        Cotton Swabs|           11|\n",
      "|Lemon Verbena Han...|           11|\n",
      "|100% Whole Wheat ...|            3|\n",
      "|Organic Bread wit...|            3|\n",
      "|     Sourdough Bread|            3|\n",
      "|Grain Free Chicke...|            8|\n",
      "|Double Duty Advan...|            8|\n",
      "|24/7 Performance ...|            8|\n",
      "|  Organic Whole Milk|           16|\n",
      "|     Grated Parmesan|           16|\n",
      "|Organic Whole Str...|           16|\n",
      "|     Sauvignon Blanc|            5|\n",
      "|  Cabernet Sauvignon|            5|\n",
      "|          Chardonnay|            5|\n",
      "|Baby Food Stage 2...|           18|\n",
      "|Spinach Peas & Pe...|           18|\n",
      "|Gluten Free Spong...|           18|\n",
      "|100% Recycled Pap...|           17|\n",
      "|Sustainably Soft ...|           17|\n",
      "|       Aluminum Foil|           17|\n",
      "|      Taco Seasoning|            6|\n",
      "|Organic Sea Salt ...|            6|\n",
      "|New Mexico Taco S...|            6|\n",
      "|Lightly Salted Ba...|           19|\n",
      "|Pretzel Crisps Or...|           19|\n",
      "|Original Veggie S...|           19|\n",
      "|Organic Tomato Ba...|            9|\n",
      "|      Marinara Sauce|            9|\n",
      "|         Basil Pesto|            9|\n",
      "|         Blueberries|            1|\n",
      "|Organic Broccoli ...|            1|\n",
      "|Organic Whole Str...|            1|\n",
      "|     Original Hummus|           20|\n",
      "|Uncured Genoa Salami|           20|\n",
      "|Organic Extra Fir...|           20|\n",
      "|         Dried Mango|           10|\n",
      "| Organic Rolled Oats|           10|\n",
      "|Organic Black Mis...|           10|\n",
      "|              Banana|            4|\n",
      "|Organic Strawberries|            4|\n",
      "|Bag of Organic Ba...|            4|\n",
      "|Boneless Skinless...|           12|\n",
      "|Ground Turkey Breast|           12|\n",
      "|Boneless Skinless...|           12|\n",
      "|Extra Virgin Oliv...|           13|\n",
      "|Creamy Peanut Butter|           13|\n",
      "|Organic Creamy Pe...|           13|\n",
      "|  Honey Nut Cheerios|           14|\n",
      "|Organic Old Fashi...|           14|\n",
      "|  Raisin Bran Cereal|           14|\n",
      "|Organic Riced Cau...|           21|\n",
      "|Uncured Beef Hot Dog|           21|\n",
      "|Organic Celery Bunch|           21|\n",
      "|Roasted Almond Bu...|            2|\n",
      "|Light CocoWhip! C...|            2|\n",
      "|    93/7 Ground Beef|            2|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Quels sont les 3 produits les plus achetés dans chaque département?\n",
    "spark.sql(\n",
    "    'SELECT ra.product_name, ra.department_id' \n",
    "    ' FROM (' \n",
    "        ' SELECT ap.product_name, ap.department_id, ap.numTimeBought, Rank() '\n",
    "        ' over (PARTITION BY ap.department_id ORDER BY ap.numTimeBought DESC) AS Rank'\n",
    "        ' FROM (SELECT p.product_name, p.department_id, SUM(op.add_to_cart_order) AS numTimeBought FROM products p'\n",
    "            ' INNER JOIN order_prod op ON p.product_id = op.product_id'\n",
    "            ' GROUP BY p.department_id, p.product_name) AS ap) AS ra'\n",
    "        ' WHERE Rank <= 3'\n",
    ").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|order_dow|      avg(sumCart)|\n",
      "+---------+------------------+\n",
      "|        0|108.93103950482433|\n",
      "|        1| 91.54849532330215|\n",
      "|        2|  84.5784477945282|\n",
      "|        3| 82.71766430802576|\n",
      "|        4| 81.92299016229087|\n",
      "|        5| 87.07221647707686|\n",
      "|        6| 97.87334003491878|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Quelle est la taille moyenne du panier pour chaque jour de la semaine?\n",
    "#utilisez un barplot pour visualiser vos résultats\n",
    "dataHistogram = spark.sql(\n",
    "    'SELECT o.order_dow, AVG(ct.sumCart)'\n",
    "    ' FROM (SELECT op.order_id, SUM(op.add_to_cart_order) AS sumCart'\n",
    "        ' FROM order_prod op'\n",
    "        ' GROUP BY op.order_id) AS ct'\n",
    "    ' INNER JOIN orders o ON o.order_id=ct.order_id'\n",
    "    ' GROUP BY o.order_dow'\n",
    "    ' ORDER BY o.order_dow'\n",
    ")\n",
    "\n",
    "dataHistogram.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ff919827b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataHistogram.toPandas().plot.bar(x=\"order_dow\", y=\"avg(sumCart)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEWqTH1QMa0a"
   },
   "source": [
    "### 3.2 MBA pour le training set (15 points)\n",
    "\n",
    "En utilisant les commandes du ``order_products__train.csv``, créez un bloc de données où chaque ligne contient la colonne ``transaction`` avec la liste des produits achetés, de manière similaire à le toy dataset. Ensuite, exécutez l'algorithme MBA pour cet ensemble de transactions.\n",
    "\n",
    "- Vous devez signaler le temps passé pour effectuer cette tâche.\n",
    "- La sortie doit contenir le nom des produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "vxZh_f3hMa0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 380 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: create a query to create and sctruct the transactions\n",
    "\"\"\"\n",
    "orders_rdd =spark.sql(\n",
    "    'SELECT op.order_id, COLLECT_LIST(p.product_name) AS transaction '\n",
    "    'FROM order_prod op '\n",
    "    'INNER JOIN products p ON op.product_id == p.product_id '\n",
    "    'GROUP BY op.order_id'\n",
    ").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "dewN0YUEMa0h"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 133.0 failed 1 times, most recent failure: Lost task 1.0 in stage 133.0 (TID 3219) (LAPTOP-7HBT1FLH.mshome.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 594, in process\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 418, in func\n    return f(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-8-e58214a7d0bd>\", line 9, in map_to_subpatterns\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u2011' in position 54: character maps to <undefined>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 594, in process\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 418, in func\n    return f(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-8-e58214a7d0bd>\", line 9, in map_to_subpatterns\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u2011' in position 54: character maps to <undefined>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \"\"\"\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1231\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 133.0 failed 1 times, most recent failure: Lost task 1.0 in stage 133.0 (TID 3219) (LAPTOP-7HBT1FLH.mshome.net executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 594, in process\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 418, in func\n    return f(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-8-e58214a7d0bd>\", line 9, in map_to_subpatterns\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u2011' in position 54: character maps to <undefined>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 594, in process\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 418, in func\n    return f(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\Julie\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-8-e58214a7d0bd>\", line 9, in map_to_subpatterns\n  File \"C:\\Users\\Julie\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u2011' in position 54: character maps to <undefined>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: run the MBA algorithm and show the first 5 association rules\n",
    "\"\"\"\n",
    "\n",
    "patterns_rdd = orders_rdd.flatMap(map_to_patterns_from_list)\n",
    "\n",
    "combined_patterns_rdd =patterns_rdd.reduceByKey(add)\n",
    "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
    "combined_rules = subpatterns_rdd.map(lambda row: (row.subpatterns, [ row.combined_rules ])).reduceByKey(add)\n",
    "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
    "\n",
    "assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DkNPEtGMa0l"
   },
   "source": [
    "# 3.3 MBA pour le dataset complet (15 points)\n",
    "\n",
    "Comme vous l'avez probablement remarqué, même pour un ensemble de données moins volumineux (le training dataset ne contient que 131 000 commandes), l'algorithme MBA est coûteux en calcul. Pour cette raison, cette fois, nous allons répéter le processus, mais en utilisant maintenant Amazon Web Services (AWS) pour créer un grand cluster. Toutes les instructions pour créer un cluster avec spark et comment soumettre un travail seront expliquées dans le laboratoire. Dans tous les cas, vous devez lire les instructions données dans le ``Instruction_AWS.pdf``.\n",
    "\n",
    "Cette fois, nous travaillerons avec le fichier ``order_products__prior.csv``, qui contient plus de 3M commandes.\n",
    "\n",
    "**PRODUCTION ATTENDUE**\n",
    "\n",
    "Après avoir exécuté le MBA pour la plus grande collection de commandes, sélectionnez au hasard UN produit acheté dans ``order_products__prior`` et affichez les règles d'association (nom du produit et valeur d'association) de ce produit, c'est-à-dire lorsque le produit est seul dans le panier. La sortie doit être formatée dans un tableau, où chaque ligne contenant les informations d'un produit associé. \n",
    "\n",
    "- Affichez l'ID et le nom du produit sélectionné au hasard.\n",
    "- Signaler le temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "X1cVWxraMa0l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_products__train.csv\n",
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|       1|     49302|                1|        1|\n",
      "|       1|     11109|                2|        1|\n",
      "|       1|     10246|                3|        0|\n",
      "|       1|     49683|                4|        0|\n",
      "|       1|     43633|                5|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orders.csv\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 2539329|      1|   prior|           1|        2|                8|                  null|\n",
      "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "products.csv\n",
      "+----------+--------------------+--------+-------------+\n",
      "|product_id|        product_name|aisle_id|department_id|\n",
      "+----------+--------------------+--------+-------------+\n",
      "|         1|Chocolate Sandwic...|      61|           19|\n",
      "|         2|    All-Seasons Salt|     104|           13|\n",
      "|         3|Robust Golden Uns...|      94|            7|\n",
      "|         4|Smart Ones Classi...|      38|            1|\n",
      "|         5|Green Chile Anyti...|       5|           13|\n",
      "+----------+--------------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: create a query to create and sctruct the transactions from the order_products__prior.csv file\n",
    "\"\"\"\n",
    "df_order_prod = spark.read.csv('instacart/order_products__train.csv', header=True, sep=',', inferSchema=True)\n",
    "print('order_products__train.csv')\n",
    "df_order_prod.show(5)\n",
    "\n",
    "df_orders = spark.read.csv('instacart/orders.csv', header=True, sep=',', inferSchema=True)\n",
    "print('orders.csv')\n",
    "df_orders.show(5)\n",
    "\n",
    "df_products = spark.read.csv('instacart/products.csv', header=True, sep=',', inferSchema=True)\n",
    "print('products.csv')\n",
    "df_products.show(5)\n",
    "\n",
    "\n",
    "orders_rdd =spark.sql(\n",
    "    'SELECT op.order_id, COLLECT_LIST(p.product_name) AS transaction '\n",
    "    'FROM order_prod op '\n",
    "    'INNER JOIN products p ON op.product_id == p.product_id '\n",
    "    'GROUP BY op.order_id'\n",
    ").rdd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVBQ12b2Ma0o"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "TODO: run the MBA algorithm and print the requested output\n",
    "\"\"\"\n",
    "combined_patterns_rdd =patterns_rdd.reduceByKey(add)\n",
    "subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n",
    "combined_rules = subpatterns_rdd.map(lambda row: (row.subpatterns, [ row.combined_rules ])).reduceByKey(add)\n",
    "assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n",
    "\n",
    "assoc_rules.map(format_tuples).takeSample(False, 1, seed=0).toDF(['patterns', 'association_rules']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Bonus - Empreinte carbone (1 point)\n",
    "\n",
    "De nos jours, en apprentissage automatique, et en particulier pour le traitement du langage naturel (NLP), il est courant d'entraîner de grands réseaux de neurones sur des ensembles de données massifs. Par exemple, il faut 78 ans pour entraîner le modèle GPT-3, un réseau de neurones de pointe en NLP avec 175 milliards de paramètres, en utilisant un seul GPU V100 \\([Wolff Anthony et al, 2020](https://arxiv.org/pdf/2007.03051.pdf)\\). L'émission de CO2 pour entraîner ce modèle équivaut à conduire une voiture au Canada sur environ 550 000 km ! Étant donné que ces environnements à forte intensité énergétique sont devenus populaires et se sont développés ces dernières années, l'apprentissage automatique (ML) pourrait commencer à devenir un acteur important du changement climatique.\n",
    " \n",
    "Il est maintenant temps de mesurer les émissions de carbone générées dans ce TP sur la base de [Strubell et al 2019](https://arxiv.org/pdf/1906.02243.pdf). Nous supposons que les machines sont situées aux États-Unis et que les deux processeurs (2,3 GHz Intel Xeon® E5-2686 v4) et la DRAM (8 GB) consomment en moyenne 243 watts par heure.\n",
    "\n",
    "\n",
    "**Vous devez exécuter la cellule ci-dessous avec le nombre d'heures passées dans AWS pour recevoir le point.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def compute_co2_emission(x):\n",
    "    \"\"\"\n",
    "    x = number of hours\n",
    "    \"\"\"\n",
    "    kwh = (243.0 * x)/1000\n",
    "    #\n",
    "    co2lbs = 0.954 * (1.58 * kwh)\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "        <h4>In this TP, you generated {co2lbs} CO2e (lbs) which is equivalent to:  </h4>\n",
    "        </br>\n",
    "        <div>\n",
    "            <ul>\n",
    "              <li> {round(co2lbs/6.61,2)} servings of beef 🐄 (4 oz. meat per serving)</li>\n",
    "              <li> {round(co2lbs * 0.337307,2)} km  of driving a car 🚗 in Canada</li>\n",
    "            </ul> \n",
    "        </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    \n",
    "hours = #TODO: Insert the number of hours spent in AWS\n",
    "compute_co2_emission(hours)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
